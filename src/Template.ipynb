{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uR1JW69eLfG_"
   },
   "source": [
    "# IF3170 Artificial Intelligence | Tugas Besar 2\n",
    "\n",
    "This notebook serves as a template for the assignment. Please create a copy of this notebook to complete your work. You can add more code blocks, markdown blocks, or new sections if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucbaI5rBLtjJ"
   },
   "source": [
    "Group Number: 20\n",
    "\n",
    "Group Members:\n",
    "- Ahmad Naufal Ramadan (13522005)\n",
    "- Kristo Anugrah (13522024)\n",
    "- Tazkia Nizami (13522032)\n",
    "- Farhan Nafis Rayhan (13522037)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwzsfETHLfHA"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "jZJU5W_4LfHB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKbjLIdYLfHC"
   },
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-IWFJ-gdLfHD"
   },
   "outputs": [],
   "source": [
    "additional_features_df = pd.read_csv('../dataset/train/additional_features_train.csv')\n",
    "basic_features_df = pd.read_csv('../dataset/train/basic_features_train.csv')\n",
    "content_features_df = pd.read_csv('../dataset/train/content_features_train.csv')\n",
    "flow_features_df = pd.read_csv('../dataset/train/flow_features_train.csv')\n",
    "labels_df = pd.read_csv('../dataset/train/labels_train.csv')\n",
    "time_features_df = pd.read_csv('../dataset/train/time_features_train.csv')\n",
    "\n",
    "data_with_label = pd.merge(basic_features_df, additional_features_df, on=\"id\")\n",
    "data_with_label = pd.merge(data_with_label, content_features_df, on=\"id\")\n",
    "data_with_label = pd.merge(data_with_label, flow_features_df, on=\"id\")\n",
    "data_with_label = pd.merge(data_with_label, time_features_df, on=\"id\")\n",
    "data_with_label = pd.merge(data_with_label, labels_df, on=\"id\")\n",
    "data = data_with_label.drop('label', axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdSor5sdIYGs"
   },
   "source": [
    "# Exploratory Data Analysis (Optional)\n",
    "\n",
    "Exploratory Data Analysis (EDA) is a crucial step in the data analysis process that involves examining and visualizing data sets to uncover patterns, trends, anomalies, and insights. It is the first step before applying more advanced statistical and machine learning techniques. EDA helps you to gain a deep understanding of the data you are working with, allowing you to make informed decisions and formulate hypotheses for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGiGPVYNIoWk"
   },
   "outputs": [],
   "source": [
    "categorical_features = data.select_dtypes(include=['object']).columns.tolist()\n",
    "binary_features = ['is_sm_ips_ports', 'is_ftp_login']\n",
    "\n",
    "numerical_features = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "for feature in binary_features:\n",
    "    numerical_features.remove(feature)\n",
    "numerical_features.remove('id')\n",
    "\n",
    "markdown_content = \"### Total Features\\n\"\n",
    "markdown_content += f\"* Total features: {len(categorical_features) + len(numerical_features) + len(binary_features)}\\n\"\n",
    "markdown_content += f\"* Total categorical features: {len(categorical_features)}\\n\"\n",
    "markdown_content += f\"* Total numerical features: {len(numerical_features)}\\n\"\n",
    "markdown_content += f\"* Total binary features: {len(binary_features)}\\n\"\n",
    "markdown_content += \"\\n### Categorical Features\\n\"\n",
    "markdown_content += \"\\n\".join([f\"* {feature}\" for feature in categorical_features])\n",
    "markdown_content += \"\\n\\n### Numerical Features\\n\"\n",
    "markdown_content += \"\\n\".join([f\"* {feature}\" for feature in numerical_features])\n",
    "markdown_content += \"\\n\\n### Binary Features\\n\"\n",
    "markdown_content += \"\\n\".join([f\"* {feature}\" for feature in binary_features])\n",
    "\n",
    "display(Markdown(markdown_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_content = \"### Categorical Features Values\\n\"\n",
    "for feature in categorical_features:\n",
    "    markdown_content += f\"\\n#### {feature}\\n\"\n",
    "    markdown_content += f\"* Unique values: {data[feature].nunique()}\\n\"\n",
    "    markdown_content += f\"* Values: {data[feature].unique()}\\n\"\n",
    "\n",
    "display(Markdown(markdown_content))\n",
    "\n",
    "markdown_content = \"### Numerical Features Values\\n\"\n",
    "for feature in numerical_features:\n",
    "    markdown_content += f\"\\n#### {feature}\\n\"\n",
    "    unique_values = data[feature].nunique()\n",
    "    if unique_values > 150:\n",
    "        markdown_content += f\"* Min: {data[feature].min()}\\n\"\n",
    "        markdown_content += f\"* Mean: {data[feature].mean()}\\n\"\n",
    "        markdown_content += f\"* Max: {data[feature].max()}\\n\"\n",
    "        markdown_content += f\"* STD: {data[feature].std()}\\n\"\n",
    "        markdown_content += f\"* 25%: {data[feature].quantile(0.25)}\\n\"\n",
    "        markdown_content += f\"* 50%: {data[feature].quantile(0.50)}\\n\"\n",
    "        markdown_content += f\"* 75%: {data[feature].quantile(0.75)}\\n\"\n",
    "    else:\n",
    "        markdown_content += f\"* Unique values: {unique_values}\\n\"\n",
    "        markdown_content += f\"* Values: {data[feature].unique()}\\n\"\n",
    "    \n",
    "\n",
    "display(Markdown(markdown_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, columns = data.shape\n",
    "display(Markdown(f\"### Data shape:\"))\n",
    "display(Markdown(f\"#### Number of Instances is: {rows}\"))\n",
    "display(Markdown(f\"#### Number of Features is: {columns}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_values = data.select_dtypes(include=['object'])\n",
    "binary_values = data.loc[:, binary_features]\n",
    "numerical_values = data.select_dtypes(include=['int64', 'float64']) \\\n",
    "                            .drop(binary_features, axis=1).drop('id', axis=1)\n",
    "\n",
    "display(Markdown(\"### Numerical values:\"))\n",
    "display(numerical_values)\n",
    "display(Markdown(\"### Categorical values:\"))\n",
    "display(categorical_values)\n",
    "display(Markdown(\"### Binary values:\"))\n",
    "display(binary_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"### Numerical value statistics:\"))\n",
    "display(numerical_values.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = data.isnull().sum().to_frame(\"missing_values_count\")\n",
    "display(Markdown(\"### Number of missing values in every column:\"))\n",
    "display(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_numerical(numerical_table: pd.DataFrame):\n",
    "  fig, axes = plt.subplots(9, 4, figsize=(30, 30))\n",
    "  axes = axes.flatten()\n",
    "  for index, col in enumerate(numerical_table.columns):\n",
    "    axes[index].hist(numerical_table[col].dropna(), color='skyblue', edgecolor='black')\n",
    "    axes[index].set_title(f'Distribution of {col}')\n",
    "  fig.tight_layout()\n",
    "    \n",
    "display(Markdown(\"### Histogram of every numerical features:\"))\n",
    "histogram_numerical(numerical_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_categorical(categorical_table: pd.DataFrame):\n",
    "  fig, axes = plt.subplots(4, 1, figsize=(100, 100))\n",
    "  axes = axes.flatten()\n",
    "  for index, col in enumerate(categorical_table.columns):\n",
    "    categorical_table[col].value_counts().plot(kind=\"bar\", ax=axes[index]).set_title(col)\n",
    "  fig.tight_layout()\n",
    "    \n",
    "display(Markdown(\"### Histogram of every categorical features:\"))\n",
    "histogram_categorical(categorical_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot with outliers\n",
    "\n",
    "def boxplot_all_column(numerical_table: pd.core.frame.DataFrame):\n",
    "  fig, axes = plt.subplots(9, 4, figsize = (20, 20))\n",
    "  axes = axes.flatten() \n",
    "  for index, col in enumerate(numerical_table.columns):\n",
    "    axes[index].boxplot(numerical_table[col].dropna(), False, sym=\"rs\", vert=False,  widths=0.5, positions=[0])\n",
    "    axes[index].set_title(f\"Boxplot of {col}\")\n",
    "  fig.tight_layout()\n",
    "\n",
    "display(Markdown(\"### Boxplot of every numerical features:\"))\n",
    "boxplot_all_column(numerical_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_of_outliers(numerical_table: pd.DataFrame):\n",
    "  Q1 = numerical_table.quantile(0.25)\n",
    "  Q3 = numerical_table.quantile(0.75)\n",
    "  IQR = Q3 - Q1\n",
    "  return (((numerical_table < (Q1 - 1.5 * IQR)) | (numerical_table > (Q3 + 1.5 * IQR))).sum().to_frame(\"Number of outliers\"))\n",
    "\n",
    "count_of_outliers(numerical_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "\n",
    "corr_numerical_matrix = numerical_values.corr()\n",
    "\n",
    "plt.figure(figsize=(180, 180))\n",
    "\n",
    "sn.heatmap(corr_numerical_matrix, annot=True, cmap='coolwarm', fmt='.2f', annot_kws={\"size\": 42})\n",
    "plt.xticks(fontsize=36)\n",
    "plt.yticks(fontsize=36)\n",
    "\n",
    "display(Markdown(\"### Correlation heatmap of every pair of numerical features\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_categorical_matrix = categorical_values.apply(lambda x : pd.factorize(x)[0]).corr(method='pearson', min_periods=1)\n",
    "display(Markdown(\"### Correlation heatmap of every pair of categorical features\"))\n",
    "display(sn.heatmap(corr_categorical_matrix, annot=True, cmap='coolwarm', fmt='.2f', annot_kws={\"size\": 10}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_contingency_table(categorical_table: pd.DataFrame):\n",
    "  columns = categorical_table.columns\n",
    "  for index1, col1 in enumerate(columns):\n",
    "    for index2, col2 in enumerate(columns):\n",
    "      if index2 >= index1:\n",
    "        break\n",
    "      first_feature = categorical_values.loc[:, col1]\n",
    "      second_feature = categorical_values.loc[:, col2]\n",
    "      contingency_table = pd.crosstab(first_feature, second_feature)\n",
    "      display(Markdown(f\"#### Contingency table of {col1} with {col2}:\"))\n",
    "      display(contingency_table)\n",
    "\n",
    "display_contingency_table(categorical_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvx-gT3bLfHM"
   },
   "source": [
    "# 1. Split Training Set and Validation Set\n",
    "\n",
    "Splitting the training and validation set works as an early diagnostic towards the performance of the model we train. This is done before the preprocessing steps to **avoid data leakage inbetween the sets**. If you want to use k-fold cross-validation, split the data later and do the cleaning and preprocessing separately for each split.\n",
    "\n",
    "Note: For training, you should use the data contained in the `train` folder given by the TA. The `test` data is only used for kaggle submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4yWCUFFBLfHM"
   },
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "\n",
    "X = data.drop(['attack_cat'], axis=1)\n",
    "y = data['attack_cat']\n",
    "\n",
    "X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "display(Markdown(\"### X train:\"))\n",
    "display(X_train)\n",
    "display(Markdown(\"### X validation:\"))\n",
    "display(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IC14lmo_LfHN"
   },
   "source": [
    "# 2. Data Cleaning and Preprocessing\n",
    "\n",
    "This step is the first thing to be done once a Data Scientist have grasped a general knowledge of the data. Raw data is **seldom ready for training**, therefore steps need to be taken to clean and format the data for the Machine Learning model to interpret.\n",
    "\n",
    "By performing data cleaning and preprocessing, you ensure that your dataset is ready for model training, leading to more accurate and reliable machine learning results. These steps are essential for transforming raw data into a format that machine learning algorithms can effectively learn from and make predictions.\n",
    "\n",
    "We will give some common methods for you to try, but you only have to **at least implement one method for each process**. For each step that you will do, **please explain the reason why did you do that process. Write it in a markdown cell under the code cell you wrote.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p95_A8hSLfHY"
   },
   "source": [
    "## A. Data Cleaning\n",
    "\n",
    "**Data cleaning** is the crucial first step in preparing your dataset for machine learning. Raw data collected from various sources is often messy and may contain errors, missing values, and inconsistencies. Data cleaning involves the following steps:\n",
    "\n",
    "1. **Handling Missing Data:** Identify and address missing values in the dataset. This can include imputing missing values, removing rows or columns with excessive missing data, or using more advanced techniques like interpolation.\n",
    "\n",
    "2. **Dealing with Outliers:** Identify and handle outliers, which are data points significantly different from the rest of the dataset. Outliers can be removed or transformed to improve model performance.\n",
    "\n",
    "3. **Data Validation:** Check for data integrity and consistency. Ensure that data types are correct, categorical variables have consistent labels, and numerical values fall within expected ranges.\n",
    "\n",
    "4. **Removing Duplicates:** Identify and remove duplicate rows, as they can skew the model's training process and evaluation metrics.\n",
    "\n",
    "5. **Feature Engineering**: Create new features or modify existing ones to extract relevant information. This step can involve scaling, normalizing, or encoding features for better model interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wruGao9LfHZ"
   },
   "source": [
    "### I. Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "ucZNfCkiLfHZ"
   },
   "outputs": [],
   "source": [
    "import sklearn.impute\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ImputeNumerical(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self):\n",
    "    self.numerical_columns = []\n",
    "    self.imp_mean = {}\n",
    "\n",
    "  def fit(self, X: pd.DataFrame, y):\n",
    "    self.imp_mean = sklearn.impute.SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    self.imp_mean.fit(X)\n",
    "    return self\n",
    "  \n",
    "  def transform(self, X: pd.DataFrame):\n",
    "    return pd.DataFrame(self.imp_mean.transform(X), columns=X.columns)\n",
    "  \n",
    "class ImputeCategorical(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self):\n",
    "    self.categorical_columns = []\n",
    "    self.imp_mode = {}\n",
    "\n",
    "  def fit(self, X: pd.DataFrame, y):\n",
    "    self.imp_mode = sklearn.impute.SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    self.imp_mode.fit(X)\n",
    "    return self\n",
    "  \n",
    "  def transform(self, X: pd.DataFrame):\n",
    "    return pd.DataFrame(self.imp_mode.transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgrSMcK75VY_"
   },
   "source": [
    "### II. Dealing with Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "CgbZ6Lv17Uf0"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class ReplaceOutliersWithMedian(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=3):\n",
    "        self.threshold = threshold\n",
    "        self.medians_ = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        X = X.toarray() if hasattr(X, 'toarray') else np.array(X)\n",
    "\n",
    "        self.medians_ = np.median(X, axis=0)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.medians_ is None:\n",
    "            raise ValueError(\"Transformer has not been fitted yet. Call 'fit' first.\")\n",
    "        \n",
    "        X = X.toarray() if hasattr(X, 'toarray') else np.array(X)\n",
    "        \n",
    "        X_transformed = X.copy()\n",
    "        \n",
    "        outlier_mask = np.abs(X_transformed) > self.threshold\n",
    "        X_transformed[outlier_mask] = self.medians_[np.where(outlier_mask)[1]]\n",
    "        \n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "class DropOutliers(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=3):\n",
    "        self.threshold = threshold\n",
    "        self.medians_ = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        X = X.toarray() if hasattr(X, 'toarray') else np.array(X)\n",
    "\n",
    "        self.medians_ = np.median(X, axis=0)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.medians_ is None:\n",
    "            raise ValueError(\"Transformer has not been fitted yet. Call 'fit' first.\")\n",
    "        \n",
    "        X = X.toarray() if hasattr(X, 'toarray') else np.array(X)\n",
    "        \n",
    "        outlier_mask = np.abs(X) > self.threshold\n",
    "        X_transformed = X[~np.any(outlier_mask, axis=1)]\n",
    "        \n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReplaceOutliersWithMedian + DropOutliers\n",
    "Kami menyiapkan kelas untuk menangani outliers dengan 2 cara, yaitu dengan menggantinya menggunakan median atau mendropnya sama sekali. Meskipun kedua kelas ini tidak terpakai pada pipeline final kami, kelas ini membantu kami dalam mencari konfigurasi pipeline yang menghasilkan performa nilai terbaik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aO0ZEZ-s6Lu-"
   },
   "source": [
    "### III. Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "BHCkkZ-v7iF8"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "\n",
    "class RemoveDuplicates(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X: pd.DataFrame, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame):\n",
    "        return X.drop_duplicates(keep='first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Duplicates\n",
    "Pada block kode ini kami menyiapkan kelas untuk menghilangkan duplikat. Meskipun kelas ini tidak terpakai pada pipeline final, kelas ini membantu kami dalam mencoba berbagai kombinasi pipeline yang menghasilkan performa model maksimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eycPASmMLfHa"
   },
   "source": [
    "### IV. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "UoXEV6wkLfHa"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "class FilterSelection(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y):\n",
    "        self.select_k_best = SelectKBest(score_func=f_classif, k=10)\n",
    "        self.select_k_best.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame):\n",
    "        selected_features = self.select_k_best.get_support()\n",
    "        return X.loc[:, selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "Proses pada tahap ini adalah pemilihan K kolom terbaik yang merepresentasikan keseluruhan data. Tahap ini dilakukan karena:\n",
    "1. Meningkatkan performa model: Dengan memilih K fitur/kolom terbaik, noise atau irrelevant data dapat dikurangi, yang kemudian akan meningkatkan performa model.\n",
    "2. Mengurangi overfitting: Dengan hanya menggunakan K fitur yang terbaik, proses ini akan membantu model men-generalisasi training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xw11_49xLfHb"
   },
   "source": [
    "## B. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhCgOl4xLfHb"
   },
   "source": [
    "### I. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "COef9EbCLfHb"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n",
    "\n",
    "class MinMaxFeatureScaling(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_scaled = X.copy()\n",
    "        X_scaled = pd.DataFrame(self.scaler.transform(X), columns=X.columns)\n",
    "        return X_scaled\n",
    "    \n",
    "class MaxAbsFeatureScaling(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.scaler = MaxAbsScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_scaled = X.copy()\n",
    "        X_scaled = pd.DataFrame(self.scaler.transform(X), columns=X.columns)\n",
    "        return X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min Max Scaling\n",
    "Pada tahap ini, dilakukan min max scaling pada kolom numerikal. Proses scaling ini dilakukan karena:\n",
    "1. Rentang nilai yang sama: Melakukan Min Max Scaling membuat seluruh nilai kolom berada pada rentang yang sama, yang dalam hal ini adalah rentang [0, 1]. \n",
    "2. Memastikan performa algoritma yang sensitif terhadap nilai: Algoritma seperti KNN yang sangat sensitif terhadap perhitungan jarak sangat sensitif terhadap nilai-nilai besar. Dengan melakukan min max scaling, semua fitur akan berkontribusi sama. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Absolute Scaling\n",
    "Pada tahap ini dilakukan max absolute scaling pada kolom kategorikal. Tahap ini dilakukan karena: \n",
    "1. Menjaga sparsity data: Teknik ini menjaga sparsity data. Karena kolom yang diterapkan teknik ini adalah kolom one-hot yang cenderung sparse, maka teknik ini cocok digunakan.\n",
    "2. Menjaga relasi/hubungan antar fitur: Teknik ini tidak membuat fitur menjadi sebuah rentang tertentu, yang berarti hubungan antar fitur tetap terjaga.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_Lh-4JwLfHc"
   },
   "source": [
    "### II. Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "psElSUugLfHc"
   },
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "\n",
    "class OneHotCategorical(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self):\n",
    "    self.categorical_columns = []\n",
    "    self.onehot = None\n",
    "\n",
    "  def fit(self, X: pd.DataFrame, y):\n",
    "    self.onehot = sklearn.preprocessing.OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    self.onehot.fit(X)\n",
    "    return self\n",
    "  \n",
    "  def transform(self, X: pd.DataFrame):\n",
    "    return pd.DataFrame(self.onehot.transform(X), columns=self.onehot.get_feature_names_out(), index=X.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "Pada tahap ini dilakukan one hot encoding pada kolom kategorikal. Tahap one hot encoding dilakukan karena:\n",
    "1. Mengubah nilai kategorik menjadi numerik: Banyak algoritma machine learning membutuhkan fitur numerikal. Tahap one-hot encoding mengubah nilai-nilai pada kolom kategorik menjadi numerik dengan tetap mempertahankan keunikan nilai.\n",
    "2. Mencegah kesalahan pada interpretasi nilai ordinal: One hot encoding menjaga independensi antar nilai kategorikal, dan mencegah kesalahan interpretasi nilai ordinal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKQO9wtB8Pc0"
   },
   "source": [
    "### III. Handling Imbalanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "u2BQd2XJ9W1i"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "class SMOTEImbalance(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.smote = None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.DataFrame):\n",
    "        self.smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "        self.smote.fit_resample(X, y) \n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame, y: pd.DataFrame):\n",
    "        X_resampled, y_resampled = self.smote.fit_resample(X, y)\n",
    "        return pd.DataFrame(X_resampled, columns=X.columns), pd.DataFrame(y_resampled, columns=y.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE\n",
    "Pada block kode ini kami menyiapkan kelas SMOTE untuk menangani label target yang imbalanced dengan melakukan oversampling pada data label yang underrepresented. Meskipun pada akhirnya kelas ini tidak kami masukkan ke pipeline final, kelas ini membantu kami dalam melakukan tuning untuk menemukan pipeline terbaik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTMSyUzf_-HQ"
   },
   "source": [
    "### IV. Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "HLOWHftjF1JU"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class StandardScaleNumerical(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, with_mean=True):\n",
    "    self.with_mean = with_mean\n",
    "    self.scaler = None\n",
    "\n",
    "  def fit(self, X: pd.DataFrame, y):\n",
    "    self.scaler = StandardScaler(with_mean=self.with_mean)\n",
    "    self.scaler.fit(X)\n",
    "\n",
    "    return self\n",
    "\n",
    "  def transform(self, X: pd.DataFrame):\n",
    "    return pd.DataFrame(self.scaler.transform(X), columns=self.scaler.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaling\n",
    "Pada tahap ini dilakukan standard scaling pada kolom kategorik dan numerik. Tahap standard scaling dilakukan karena:\n",
    "1. Meratakan kontribusi setiap fitur pada proses jalannya model: Proses standard scaling akan membuat kontribusi setiap fitur sama dengan mengubah rentang setiap fitur sesuai dengan Z-score masing-masing.\n",
    "2. Menjaga relasi antar fitur: Tahap standard scaling menjaga relasi antar fitur karena standard scaling tidak melakukan mapping dari setiap nilai ke sebuah rentang spesifik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSZPZSntF3Kb"
   },
   "source": [
    "### V. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "5rksSMAWICY_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ctVzt5DLfHd"
   },
   "source": [
    "# 3. Compile Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "jHraoW_7LfHd"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', ImputeNumerical()),\n",
    "    ('scaler', MinMaxFeatureScaling()),\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', ImputeCategorical()),\n",
    "    ('onehot', OneHotCategorical()),\n",
    "    ('scaler', MaxAbsFeatureScaling()),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('numerical', numerical_pipeline, [*numerical_features, *binary_features]),\n",
    "    ('categorical', categorical_pipeline, [feature for feature in categorical_features if (feature != 'attack_cat')]),\n",
    "])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('filter', SelectKBest(score_func=f_classif, k=70)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9s56aFFxLfHd"
   },
   "outputs": [],
   "source": [
    "train_set = pipe.fit_transform(X_train, y_train)\n",
    "val_set = pipe.transform(X_val)\n",
    "print(train_set.shape)\n",
    "print(val_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9A3adbZXLfHe"
   },
   "source": [
    "# 4. Modeling and Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnhMNbBILfHf"
   },
   "source": [
    "## A. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KV6ICmFmlqjk"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(train_set, y_train)\n",
    "with open('../model/knn_model_sklearn.pkl', 'wb') as model_file:\n",
    "    pickle.dump(knn, model_file)\n",
    "\n",
    "scores = cross_val_score(knn, train_set, y_train, cv=kf, scoring='accuracy')\n",
    "print(f\"Cross validation scores: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean()}\")\n",
    "print(f\"Standard deviation of accuracy: {scores.std()}\")\n",
    "\n",
    "y_pred = knn.predict(val_set)\n",
    "f1 = f1_score(y_val, y_pred, average='macro')\n",
    "print(f\"F1 score: {f1}\")\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['attack_cat'] = y_val\n",
    "df['predicted'] = y_pred\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self, _k : int, _distance_metric : str, _p = 3):\n",
    "        self.k : int = _k\n",
    "        self.distance_function = self._set_distance_function(_distance_metric.lower())\n",
    "        self.p = _p\n",
    "        self.X_train = None\n",
    "        self.Y_train = None\n",
    "\n",
    "        self.i = 0\n",
    "\n",
    "    def _set_distance_function(self, input_metric):\n",
    "        distance_functions = {\n",
    "            \"euclidean\": self._euclidean,\n",
    "            \"manhattan\": self._manhattan,\n",
    "            \"minkowski\": self._minkowski\n",
    "        }\n",
    "        if input_metric not in distance_functions:\n",
    "            raise ValueError(f\"Unsupported distance metric: {input_metric}\")\n",
    "        return distance_functions[input_metric]\n",
    "    \n",
    "    def _euclidean(self, a, b):\n",
    "        self.i += 1\n",
    "        print(self.i)\n",
    "        return np.sqrt(np.sum((a - b) ** 2, axis=1))\n",
    "\n",
    "    def _manhattan(self, a, b):\n",
    "        return np.sum(np.abs(a - b), axis=1)\n",
    "\n",
    "    def _minkowski(self, a, b):\n",
    "        return np.sum(np.abs(a - b) ** self.p, axis=1) ** (1 / self.p)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = np.array(X)\n",
    "        self.Y_train = np.array(y)\n",
    "\n",
    "    # def predict(self, X):\n",
    "    #     X = np.array(X)\n",
    "    #     predictions = [self._predict_single(x) for x in X]\n",
    "    #     return np.array(predictions)\n",
    "\n",
    "    # def _predict_single(self, x):\n",
    "    #     # distances = self.distance_function(self.X_train, x)\n",
    "    #     distances = np.array([self.distance_function(x, x_train) for x_train in self.X_train])\n",
    "    #     k_nearest_neighbors_index = np.argsort(distances)[:self.k]\n",
    "    #     k_nearest_neighbors_labels = self.Y_train[k_nearest_neighbors_index]\n",
    "\n",
    "    #     self.i += 1\n",
    "    #     print(self.i)\n",
    "\n",
    "    #     return np.bincount(k_nearest_neighbors_labels).argmax()\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        distances = np.apply_along_axis(self.distance_function, 1, X, self.X_train)\n",
    "        k_nearest_indices = np.argpartition(distances, self.k, axis=1)[:, :self.k]\n",
    "        k_nearest_labels = self.Y_train[k_nearest_indices]\n",
    "        predictions = np.apply_along_axis(self.majority_vote, 1, k_nearest_labels)\n",
    "        return predictions\n",
    "    \n",
    "    def majority_vote(self, neighbors):\n",
    "        return np.bincount(neighbors).argmax()\n",
    "    \n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        correct = np.sum(y_true == y_pred)\n",
    "        return (correct / len(y_true)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNN(131, \"manhattan\")\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn_model.predict(X_val)\n",
    "accuracy = knn_model.accuracy(y_val, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nW0bMzkDLfHf"
   },
   "source": [
    "## B. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_XwsN_-LfHg"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(train_set, y_train)\n",
    "with open('../model/nb_model_sklearn.pkl', 'wb') as model_file:\n",
    "    pickle.dump(nb, model_file)\n",
    "\n",
    "scores = cross_val_score(nb, train_set, y_train, cv=kf, scoring='accuracy')\n",
    "print(f\"Cross validation scores: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean()}\")\n",
    "print(f\"Standard deviation of accuracy: {scores.std()}\")\n",
    "\n",
    "y_pred = nb.predict(val_set)\n",
    "f1 = f1_score(y_val, y_pred, average='macro')\n",
    "print(f\"F1 score: {f1}\")\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['attack_cat'] = y_val\n",
    "df['predicted'] = y_pred\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.class_priors = None\n",
    "        self.class_means = None\n",
    "        self.class_variances = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        self.class_means = np.zeros((n_classes, n_features))\n",
    "        self.class_variances = np.zeros((n_classes, n_features))\n",
    "        self.class_priors = np.zeros(n_classes)\n",
    "\n",
    "        for i, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "            self.class_priors[i] = X_c.shape[0] / X.shape[0]\n",
    "            self.class_means[i, :] = X_c.mean(axis=0)\n",
    "            self.class_variances[i, :] = X_c.var(axis=0) + 1e-7\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _gaussian_probability(self, x, mean, variance):\n",
    "        exponent = np.exp(-((x - mean)**2 / (2 * variance)))\n",
    "        return (1 / (np.sqrt(2 * np.pi * variance))) * exponent\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        n_classes = len(self.classes)\n",
    "        \n",
    "        probabilities = np.zeros((n_samples, n_classes))\n",
    "        for i in range(n_classes):\n",
    "            prior = np.log(self.class_priors[i])\n",
    "            conditional = np.sum(np.log(\n",
    "                self._gaussian_probability(\n",
    "                    X, \n",
    "                    self.class_means[i, :], \n",
    "                    self.class_variances[i, :]\n",
    "                ) + 1e-10 \n",
    "            ), axis=1)\n",
    "            \n",
    "            \n",
    "            probabilities[:, i] = prior + conditional\n",
    "        \n",
    "        probabilities = np.exp(probabilities)\n",
    "        probabilities /= probabilities.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return self.classes[np.argmax(probabilities, axis=1)]\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomNaiveBayes()\n",
    "model.fit(train_set, y_train)\n",
    "\n",
    "with open('../model/nb_model_scratch.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "\n",
    "y_pred = model.predict(val_set)\n",
    "\n",
    "f1 = f1_score(y_val, y_pred, average='macro')\n",
    "print(f\"F1 score: {f1}\")\n",
    "\n",
    "df = pd.DataFrame({'attack_cat': y_val, 'predicted': y_pred})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLDtIkPdLfHg"
   },
   "source": [
    "## C. ID3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZ6_x1LKLfHh"
   },
   "outputs": [],
   "source": [
    "# Type your code here\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "dt.fit(train_set, y_train)\n",
    "with open('../model/id3_model_sklearn.pkl', 'wb') as model_file:\n",
    "    pickle.dump(dt, model_file)\n",
    "\n",
    "scores = cross_val_score(dt, train_set, y_train, cv=kf, scoring='accuracy')\n",
    "print(f\"Cross validation scores: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean()}\")\n",
    "print(f\"Standard deviation of accuracy: {scores.std()}\")\n",
    "\n",
    "y_pred = dt.predict(val_set)\n",
    "f1 = f1_score(y_val, y_pred, average='macro')\n",
    "print(f\"F1 score: {f1}\")\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['attack_cat'] = y_val\n",
    "df['predicted'] = y_pred\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class ID3(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, max_depth=5):\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "\n",
    "        X, y = check_X_y(X, y, dtype=np.float64)\n",
    "\n",
    "        self.tree_ = self._build_tree(X, y)\n",
    "\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        self.classes_ = le.classes_\n",
    "        self.label_encoder_ = le\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        y = np.asarray(y, dtype=np.int64)\n",
    "        \n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        return -np.sum(probabilities * np.log2(probabilities + 1e-10))\n",
    "    \n",
    "    def _information_gain(self, X, y, feature_idx):\n",
    "        total_entropy = self._entropy(y)\n",
    "        \n",
    "        unique_values = np.unique(X[:, feature_idx])\n",
    "        \n",
    "        weighted_entropy = 0\n",
    "        for value in unique_values:\n",
    "            mask = X[:, feature_idx] == value\n",
    "            sub_y = y[mask]\n",
    "            \n",
    "            prob = len(sub_y) / len(y)\n",
    "            weighted_entropy += prob * self._entropy(sub_y)\n",
    "        \n",
    "        return total_entropy - weighted_entropy\n",
    "    \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        y = np.asarray(y, dtype=np.int64)\n",
    "        \n",
    "        unique_classes = np.unique(y)\n",
    "\n",
    "        if len(unique_classes) == 1 or depth == self.max_depth or X.shape[1] == 0:\n",
    "            return int(np.argmax(np.bincount(y))) if len(y) > 0 else None\n",
    "\n",
    "        max_gain = -1\n",
    "        best_feature = None\n",
    "        for i in range(X.shape[1]):\n",
    "            gain = self._information_gain(X, y, i)\n",
    "            if gain > max_gain:\n",
    "                max_gain = gain\n",
    "                best_feature = i\n",
    "\n",
    "        if max_gain == 0:\n",
    "            return int(np.argmax(np.bincount(y)))\n",
    "        \n",
    "        node = {'feature': best_feature}\n",
    "        node['children'] = {}\n",
    "        \n",
    "        for value in np.unique(X[:, best_feature]):\n",
    "            mask = X[:, best_feature] == value\n",
    "            \n",
    "            X_subset = np.delete(X[mask], best_feature, axis=1)\n",
    "            y_subset = y[mask]\n",
    "\n",
    "            if len(y_subset) == 0:\n",
    "                continue\n",
    "            \n",
    "            subtree = self._build_tree(\n",
    "                X_subset,\n",
    "                y_subset,\n",
    "                depth=depth+1\n",
    "            )\n",
    "            \n",
    "            node['children'][value] = subtree\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = check_array(X, dtype=np.float64)\n",
    "        check_is_fitted(self, ['tree_', 'classes_'])\n",
    "        \n",
    "        predictions = [self._predict_single(x) for x in X]\n",
    "        \n",
    "        return self.label_encoder_.inverse_transform(predictions)\n",
    "    \n",
    "    def _predict_single(self, x):\n",
    "        node = self.tree_\n",
    "        \n",
    "        while isinstance(node, dict):\n",
    "            feature = node['feature']\n",
    "            value = x[feature]\n",
    "            \n",
    "            if value not in node['children']:\n",
    "                node = list(node['children'].values())[0]\n",
    "            else:\n",
    "                node = node['children'][value]\n",
    "                x = np.delete(x, feature)\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X = check_array(X, dtype=np.float64)\n",
    "        check_is_fitted(self, ['tree_', 'classes_'])\n",
    "\n",
    "        predictions = [self._predict_single(x) for x in X]\n",
    "\n",
    "        proba = np.zeros((X.shape[0], len(self.classes_)))\n",
    "        for i, p in enumerate(predictions):\n",
    "            proba[i, p] = 1\n",
    "        \n",
    "        return proba\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id3 = ID3(max_depth=12)\n",
    "id3.fit(train_set, y_train)\n",
    "with open('../model/id3_model_scratch.pkl', 'wb') as model_file:\n",
    "    pickle.dump(id3, model_file)\n",
    "\n",
    "scores = cross_val_score(id3, train_set, y_train, cv=kf, scoring='accuracy')\n",
    "print(f\"Cross validation scores: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean()}\")\n",
    "print(f\"Standard deviation of accuracy: {scores.std()}\")\n",
    "\n",
    "y_pred = id3.predict(val_set)\n",
    "f1 = f1_score(y_val, y_pred, average='macro')\n",
    "print(f\"F1 score: {f1}\")\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['attack_cat'] = y_val\n",
    "df['predicted'] = y_pred\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoH2u6fOLfHh"
   },
   "source": [
    "## D. Improvements (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "pg-A54yELfHh"
   },
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Li4l53DjLfHh"
   },
   "source": [
    "## E. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LeqnfWc-LfHi"
   },
   "outputs": [],
   "source": [
    "# Membaca semua file csv test\n",
    "additional_features_df = pd.read_csv('../dataset/test/additional_features_test.csv')\n",
    "basic_features_df = pd.read_csv('../dataset/test/basic_features_test.csv')\n",
    "content_features_df = pd.read_csv('../dataset/test/content_features_test.csv')\n",
    "flow_features_df = pd.read_csv('../dataset/test/flow_features_test.csv')\n",
    "time_features_df = pd.read_csv('../dataset/test/time_features_test.csv')\n",
    "\n",
    "# Menggabungkan data training dan testing untuk analisis EDA menyeluruh\n",
    "test_data = pd.merge(basic_features_df, additional_features_df, on=\"id\")\n",
    "test_data = pd.merge(test_data, content_features_df, on=\"id\")\n",
    "test_data = pd.merge(test_data, flow_features_df, on=\"id\")\n",
    "test_data = pd.merge(test_data, time_features_df, on=\"id\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', ImputeNumerical()),\n",
    "    ('scaler', MinMaxFeatureScaling()),\n",
    "    ('standard', StandardScaleNumerical())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', ImputeCategorical()),\n",
    "    ('onehot', OneHotCategorical()),\n",
    "    ('scaler', MaxAbsFeatureScaling()),\n",
    "    ('standard', StandardScaleNumerical(with_mean=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('numerical', numerical_pipeline, [*numerical_features, *binary_features]),\n",
    "    ('categorical', categorical_pipeline, [feature for feature in categorical_features if (feature != 'attack_cat')]),\n",
    "])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('outlier', ReplaceOutliersWithMedian()),\n",
    "    ('filter', SelectKBest(score_func=f_classif, k=15)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pipe.fit_transform(X, y)\n",
    "test_set = pipe.transform(test_data)\n",
    "print(train_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(train_set, y)\n",
    "\n",
    "y_pred_knn = knn.predict(test_set)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['id'] = test_data['id']\n",
    "df['attack_cat'] = y_pred_knn\n",
    "df.to_csv('../submission/knn_submission.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.fit(train_set, y)\n",
    "\n",
    "y_pred_nb = nb.predict(test_set)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['id'] = test_data['id']\n",
    "df['attack_cat'] = y_pred_nb\n",
    "df.to_csv('../submission/nb_submission.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(train_set, y)\n",
    "\n",
    "y_pred_dt = dt.predict(test_set)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['id'] = test_data['id']\n",
    "df['attack_cat'] = y_pred_dt\n",
    "df.to_csv('../submission/id3_submission.csv', index=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id3.fit(train_set, y)\n",
    "\n",
    "y_pred_id3 = id3.predict(test_set)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['id'] = test_data['id']\n",
    "df['attack_cat'] = y_pred_id3\n",
    "df.to_csv('../submission/id3_scratch_submission.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_scratch = CustomNaiveBayes()\n",
    "nb_scratch.fit(train_set, y)\n",
    "\n",
    "y_pred_nb_scratch = nb_scratch.predict(test_set)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['id'] = test_data['id']\n",
    "df['attack_cat'] = y_pred_nb_scratch\n",
    "df.to_csv('../submission/nb_scratch_submission.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-jXvKOpLfHi"
   },
   "source": [
    "# 6. Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWL3nEAELfHj"
   },
   "source": [
    "`Provide your analysis here`"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
