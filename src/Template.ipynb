{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uR1JW69eLfG_"
   },
   "source": [
    "# IF3170 Artificial Intelligence | Tugas Besar 2\n",
    "\n",
    "This notebook serves as a template for the assignment. Please create a copy of this notebook to complete your work. You can add more code blocks, markdown blocks, or new sections if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucbaI5rBLtjJ"
   },
   "source": [
    "Group Number: 20\n",
    "\n",
    "Group Members:\n",
    "- Ahmad Naufal Ramadan (13522005)\n",
    "- Kristo Anugrah (13522024)\n",
    "- Tazkia Nizami (13522032)\n",
    "- Farhan Nafis Rayhan (13522037)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwzsfETHLfHA"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "jZJU5W_4LfHB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKbjLIdYLfHC"
   },
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-IWFJ-gdLfHD"
   },
   "outputs": [],
   "source": [
    "additional_features_df = pd.read_csv('../dataset/train/additional_features_train.csv')\n",
    "basic_features_df = pd.read_csv('../dataset/train/basic_features_train.csv')\n",
    "content_features_df = pd.read_csv('../dataset/train/content_features_train.csv')\n",
    "flow_features_df = pd.read_csv('../dataset/train/flow_features_train.csv')\n",
    "labels_df = pd.read_csv('../dataset/train/labels_train.csv')\n",
    "time_features_df = pd.read_csv('../dataset/train/time_features_train.csv')\n",
    "\n",
    "data_with_label = pd.merge(basic_features_df, additional_features_df, on=\"id\")\n",
    "data_with_label = pd.merge(data_with_label, content_features_df, on=\"id\")\n",
    "data_with_label = pd.merge(data_with_label, flow_features_df, on=\"id\")\n",
    "data_with_label = pd.merge(data_with_label, time_features_df, on=\"id\")\n",
    "data_with_label = pd.merge(data_with_label, labels_df, on=\"id\")\n",
    "data = data_with_label.drop('label', axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdSor5sdIYGs"
   },
   "source": [
    "# Exploratory Data Analysis (Optional)\n",
    "\n",
    "Exploratory Data Analysis (EDA) is a crucial step in the data analysis process that involves examining and visualizing data sets to uncover patterns, trends, anomalies, and insights. It is the first step before applying more advanced statistical and machine learning techniques. EDA helps you to gain a deep understanding of the data you are working with, allowing you to make informed decisions and formulate hypotheses for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGiGPVYNIoWk"
   },
   "outputs": [],
   "source": [
    "categorical_features = data.select_dtypes(include=['object']).columns.tolist()\n",
    "binary_features = ['is_sm_ips_ports', 'is_ftp_login']\n",
    "\n",
    "numerical_features = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "for feature in binary_features:\n",
    "    numerical_features.remove(feature)\n",
    "numerical_features.remove('id')\n",
    "\n",
    "markdown_content = \"### Total Features\\n\"\n",
    "markdown_content += f\"* Total features: {len(categorical_features) + len(numerical_features) + len(binary_features)}\\n\"\n",
    "markdown_content += f\"* Total categorical features: {len(categorical_features)}\\n\"\n",
    "markdown_content += f\"* Total numerical features: {len(numerical_features)}\\n\"\n",
    "markdown_content += f\"* Total binary features: {len(binary_features)}\\n\"\n",
    "markdown_content += \"\\n### Categorical Features\\n\"\n",
    "markdown_content += \"\\n\".join([f\"* {feature}\" for feature in categorical_features])\n",
    "markdown_content += \"\\n\\n### Numerical Features\\n\"\n",
    "markdown_content += \"\\n\".join([f\"* {feature}\" for feature in numerical_features])\n",
    "markdown_content += \"\\n\\n### Binary Features\\n\"\n",
    "markdown_content += \"\\n\".join([f\"* {feature}\" for feature in binary_features])\n",
    "\n",
    "display(Markdown(markdown_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_content = \"### Categorical Features Values\\n\"\n",
    "for feature in categorical_features:\n",
    "    markdown_content += f\"\\n#### {feature}\\n\"\n",
    "    markdown_content += f\"* Unique values: {data[feature].nunique()}\\n\"\n",
    "    markdown_content += f\"* Values: {data[feature].unique()}\\n\"\n",
    "\n",
    "display(Markdown(markdown_content))\n",
    "\n",
    "markdown_content = \"### Numerical Features Values\\n\"\n",
    "for feature in numerical_features:\n",
    "    markdown_content += f\"\\n#### {feature}\\n\"\n",
    "    unique_values = data[feature].nunique()\n",
    "    if unique_values > 150:\n",
    "        markdown_content += f\"* Min: {data[feature].min()}\\n\"\n",
    "        markdown_content += f\"* Mean: {data[feature].mean()}\\n\"\n",
    "        markdown_content += f\"* Max: {data[feature].max()}\\n\"\n",
    "        markdown_content += f\"* STD: {data[feature].std()}\\n\"\n",
    "        markdown_content += f\"* 25%: {data[feature].quantile(0.25)}\\n\"\n",
    "        markdown_content += f\"* 50%: {data[feature].quantile(0.50)}\\n\"\n",
    "        markdown_content += f\"* 75%: {data[feature].quantile(0.75)}\\n\"\n",
    "    else:\n",
    "        markdown_content += f\"* Unique values: {unique_values}\\n\"\n",
    "        markdown_content += f\"* Values: {data[feature].unique()}\\n\"\n",
    "    \n",
    "\n",
    "display(Markdown(markdown_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, columns = data.shape\n",
    "display(Markdown(f\"### Data shape:\"))\n",
    "display(Markdown(f\"#### Number of Instances is: {rows}\"))\n",
    "display(Markdown(f\"#### Number of Features is: {columns}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_values = data.select_dtypes(include=['object'])\n",
    "binary_values = data.loc[:, binary_features]\n",
    "numerical_values = data.select_dtypes(include=['int64', 'float64']) \\\n",
    "                            .drop(binary_features, axis=1).drop('id', axis=1)\n",
    "\n",
    "display(Markdown(\"### Numerical values:\"))\n",
    "display(numerical_values)\n",
    "display(Markdown(\"### Categorical values:\"))\n",
    "display(categorical_values)\n",
    "display(Markdown(\"### Binary values:\"))\n",
    "display(binary_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"### Numerical value statistics:\"))\n",
    "display(numerical_values.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = data.isnull().sum().to_frame(\"missing_values_count\")\n",
    "display(Markdown(\"### Number of missing values in every column:\"))\n",
    "display(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_numerical(numerical_table: pd.DataFrame):\n",
    "  fig, axes = plt.subplots(9, 4, figsize=(30, 30))\n",
    "  axes = axes.flatten()\n",
    "  for index, col in enumerate(numerical_table.columns):\n",
    "    axes[index].hist(numerical_table[col].dropna(), color='skyblue', edgecolor='black')\n",
    "    axes[index].set_title(f'Distribution of {col}')\n",
    "  fig.tight_layout()\n",
    "    \n",
    "display(Markdown(\"### Histogram of every numerical features:\"))\n",
    "histogram_numerical(numerical_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_categorical(categorical_table: pd.DataFrame):\n",
    "  fig, axes = plt.subplots(4, 1, figsize=(100, 100))\n",
    "  axes = axes.flatten()\n",
    "  for index, col in enumerate(categorical_table.columns):\n",
    "    categorical_table[col].value_counts().plot(kind=\"bar\", ax=axes[index]).set_title(col)\n",
    "  fig.tight_layout()\n",
    "    \n",
    "display(Markdown(\"### Histogram of every categorical features:\"))\n",
    "histogram_categorical(categorical_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot with outliers\n",
    "\n",
    "def boxplot_all_column(numerical_table: pd.core.frame.DataFrame):\n",
    "  fig, axes = plt.subplots(9, 4, figsize = (20, 20))\n",
    "  axes = axes.flatten() \n",
    "  for index, col in enumerate(numerical_table.columns):\n",
    "    axes[index].boxplot(numerical_table[col].dropna(), False, sym=\"rs\", vert=False,  widths=0.5, positions=[0])\n",
    "    axes[index].set_title(f\"Boxplot of {col}\")\n",
    "  fig.tight_layout()\n",
    "\n",
    "display(Markdown(\"### Boxplot of every numerical features:\"))\n",
    "boxplot_all_column(numerical_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_of_outliers(numerical_table: pd.DataFrame):\n",
    "  Q1 = numerical_table.quantile(0.25)\n",
    "  Q3 = numerical_table.quantile(0.75)\n",
    "  IQR = Q3 - Q1\n",
    "  return (((numerical_table < (Q1 - 1.5 * IQR)) | (numerical_table > (Q3 + 1.5 * IQR))).sum().to_frame(\"Number of outliers\"))\n",
    "\n",
    "count_of_outliers(numerical_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "\n",
    "corr_numerical_matrix = numerical_values.corr()\n",
    "\n",
    "plt.figure(figsize=(180, 180))\n",
    "\n",
    "sn.heatmap(corr_numerical_matrix, annot=True, cmap='coolwarm', fmt='.2f', annot_kws={\"size\": 42})\n",
    "plt.xticks(fontsize=36)\n",
    "plt.yticks(fontsize=36)\n",
    "\n",
    "display(Markdown(\"### Correlation heatmap of every pair of numerical features\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_categorical_matrix = categorical_values.apply(lambda x : pd.factorize(x)[0]).corr(method='pearson', min_periods=1)\n",
    "display(Markdown(\"### Correlation heatmap of every pair of categorical features\"))\n",
    "display(sn.heatmap(corr_categorical_matrix, annot=True, cmap='coolwarm', fmt='.2f', annot_kws={\"size\": 10}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_contingency_table(categorical_table: pd.DataFrame):\n",
    "  columns = categorical_table.columns\n",
    "  for index1, col1 in enumerate(columns):\n",
    "    for index2, col2 in enumerate(columns):\n",
    "      if index2 >= index1:\n",
    "        break\n",
    "      first_feature = categorical_values.loc[:, col1]\n",
    "      second_feature = categorical_values.loc[:, col2]\n",
    "      contingency_table = pd.crosstab(first_feature, second_feature)\n",
    "      display(Markdown(f\"#### Contingency table of {col1} with {col2}:\"))\n",
    "      display(contingency_table)\n",
    "\n",
    "display_contingency_table(categorical_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvx-gT3bLfHM"
   },
   "source": [
    "# 1. Split Training Set and Validation Set\n",
    "\n",
    "Splitting the training and validation set works as an early diagnostic towards the performance of the model we train. This is done before the preprocessing steps to **avoid data leakage inbetween the sets**. If you want to use k-fold cross-validation, split the data later and do the cleaning and preprocessing separately for each split.\n",
    "\n",
    "Note: For training, you should use the data contained in the `train` folder given by the TA. The `test` data is only used for kaggle submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4yWCUFFBLfHM"
   },
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "\n",
    "X = data.drop(['attack_cat'], axis=1)\n",
    "y = data['attack_cat']\n",
    "\n",
    "X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "display(Markdown(\"### X train:\"))\n",
    "display(X_train)\n",
    "display(Markdown(\"### X validation:\"))\n",
    "display(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IC14lmo_LfHN"
   },
   "source": [
    "# 2. Data Cleaning and Preprocessing\n",
    "\n",
    "This step is the first thing to be done once a Data Scientist have grasped a general knowledge of the data. Raw data is **seldom ready for training**, therefore steps need to be taken to clean and format the data for the Machine Learning model to interpret.\n",
    "\n",
    "By performing data cleaning and preprocessing, you ensure that your dataset is ready for model training, leading to more accurate and reliable machine learning results. These steps are essential for transforming raw data into a format that machine learning algorithms can effectively learn from and make predictions.\n",
    "\n",
    "We will give some common methods for you to try, but you only have to **at least implement one method for each process**. For each step that you will do, **please explain the reason why did you do that process. Write it in a markdown cell under the code cell you wrote.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p95_A8hSLfHY"
   },
   "source": [
    "## A. Data Cleaning\n",
    "\n",
    "**Data cleaning** is the crucial first step in preparing your dataset for machine learning. Raw data collected from various sources is often messy and may contain errors, missing values, and inconsistencies. Data cleaning involves the following steps:\n",
    "\n",
    "1. **Handling Missing Data:** Identify and address missing values in the dataset. This can include imputing missing values, removing rows or columns with excessive missing data, or using more advanced techniques like interpolation.\n",
    "\n",
    "2. **Dealing with Outliers:** Identify and handle outliers, which are data points significantly different from the rest of the dataset. Outliers can be removed or transformed to improve model performance.\n",
    "\n",
    "3. **Data Validation:** Check for data integrity and consistency. Ensure that data types are correct, categorical variables have consistent labels, and numerical values fall within expected ranges.\n",
    "\n",
    "4. **Removing Duplicates:** Identify and remove duplicate rows, as they can skew the model's training process and evaluation metrics.\n",
    "\n",
    "5. **Feature Engineering**: Create new features or modify existing ones to extract relevant information. This step can involve scaling, normalizing, or encoding features for better model interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wruGao9LfHZ"
   },
   "source": [
    "### I. Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "ucZNfCkiLfHZ"
   },
   "outputs": [],
   "source": [
    "import sklearn.impute\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ImputeNumerical(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self):\n",
    "    self.numerical_columns = []\n",
    "    self.imp_mean = {}\n",
    "\n",
    "  def fit(self, X: pd.DataFrame, y):\n",
    "    self.imp_mean = sklearn.impute.SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    self.imp_mean.fit(X)\n",
    "    return self\n",
    "  \n",
    "  def transform(self, X: pd.DataFrame):\n",
    "    return pd.DataFrame(self.imp_mean.transform(X), columns=X.columns)\n",
    "  \n",
    "class ImputeCategorical(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self):\n",
    "    self.categorical_columns = []\n",
    "    self.imp_mode = {}\n",
    "\n",
    "  def fit(self, X: pd.DataFrame, y):\n",
    "    self.imp_mode = sklearn.impute.SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    self.imp_mode.fit(X)\n",
    "    return self\n",
    "  \n",
    "  def transform(self, X: pd.DataFrame):\n",
    "    return pd.DataFrame(self.imp_mode.transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgrSMcK75VY_"
   },
   "source": [
    "### II. Dealing with Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "CgbZ6Lv17Uf0"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class ReplaceOutliersWithMedian(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=3):\n",
    "        self.threshold = threshold\n",
    "        self.medians_ = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        X = X.toarray() if hasattr(X, 'toarray') else np.array(X)\n",
    "\n",
    "        self.medians_ = np.median(X, axis=0)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.medians_ is None:\n",
    "            raise ValueError(\"Transformer has not been fitted yet. Call 'fit' first.\")\n",
    "        \n",
    "        X = X.toarray() if hasattr(X, 'toarray') else np.array(X)\n",
    "        \n",
    "        X_transformed = X.copy()\n",
    "        \n",
    "        outlier_mask = np.abs(X_transformed) > self.threshold\n",
    "        X_transformed[outlier_mask] = self.medians_[np.where(outlier_mask)[1]]\n",
    "        \n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "class DropOutliers(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=3):\n",
    "        self.threshold = threshold\n",
    "        self.medians_ = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        X = X.toarray() if hasattr(X, 'toarray') else np.array(X)\n",
    "\n",
    "        self.medians_ = np.median(X, axis=0)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.medians_ is None:\n",
    "            raise ValueError(\"Transformer has not been fitted yet. Call 'fit' first.\")\n",
    "        \n",
    "        X = X.toarray() if hasattr(X, 'toarray') else np.array(X)\n",
    "        \n",
    "        outlier_mask = np.abs(X) > self.threshold\n",
    "        X_transformed = X[~np.any(outlier_mask, axis=1)]\n",
    "        \n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReplaceOutliersWithMedian + DropOutliers\n",
    "Kami menyiapkan kelas untuk menangani outliers dengan 2 cara, yaitu dengan menggantinya menggunakan median atau mendropnya sama sekali. Meskipun kedua kelas ini tidak terpakai pada pipeline final kami, kelas ini membantu kami dalam mencari konfigurasi pipeline yang menghasilkan performa nilai terbaik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aO0ZEZ-s6Lu-"
   },
   "source": [
    "### III. Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "BHCkkZ-v7iF8"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "\n",
    "class RemoveDuplicates(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X: pd.DataFrame, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame):\n",
    "        return X.drop_duplicates(keep='first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Duplicates\n",
    "Pada block kode ini kami menyiapkan kelas untuk menghilangkan duplikat. Meskipun kelas ini tidak terpakai pada pipeline final, kelas ini membantu kami dalam mencoba berbagai kombinasi pipeline yang menghasilkan performa model maksimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eycPASmMLfHa"
   },
   "source": [
    "### IV. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "UoXEV6wkLfHa"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "class FilterSelection(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y):\n",
    "        self.select_k_best = SelectKBest(score_func=f_classif, k=10)\n",
    "        self.select_k_best.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame):\n",
    "        selected_features = self.select_k_best.get_support()\n",
    "        return X.loc[:, selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "Proses pada tahap ini adalah pemilihan K kolom terbaik yang merepresentasikan keseluruhan data. Tahap ini dilakukan karena:\n",
    "1. Meningkatkan performa model: Dengan memilih K fitur/kolom terbaik, noise atau irrelevant data dapat dikurangi, yang kemudian akan meningkatkan performa model.\n",
    "2. Mengurangi overfitting: Dengan hanya menggunakan K fitur yang terbaik, proses ini akan membantu model men-generalisasi training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xw11_49xLfHb"
   },
   "source": [
    "## B. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhCgOl4xLfHb"
   },
   "source": [
    "### I. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "COef9EbCLfHb"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n",
    "\n",
    "class MinMaxFeatureScaling(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_scaled = X.copy()\n",
    "        X_scaled = pd.DataFrame(self.scaler.transform(X), columns=X.columns)\n",
    "        return X_scaled\n",
    "    \n",
    "class MaxAbsFeatureScaling(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.scaler = MaxAbsScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_scaled = X.copy()\n",
    "        X_scaled = pd.DataFrame(self.scaler.transform(X), columns=X.columns)\n",
    "        return X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min Max Scaling\n",
    "Pada tahap ini, dilakukan min max scaling pada kolom numerikal. Proses scaling ini dilakukan karena:\n",
    "1. Rentang nilai yang sama: Melakukan Min Max Scaling membuat seluruh nilai kolom berada pada rentang yang sama, yang dalam hal ini adalah rentang [0, 1]. \n",
    "2. Memastikan performa algoritma yang sensitif terhadap nilai: Algoritma seperti KNN yang sangat sensitif terhadap perhitungan jarak sangat sensitif terhadap nilai-nilai besar. Dengan melakukan min max scaling, semua fitur akan berkontribusi sama. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Absolute Scaling\n",
    "Pada tahap ini dilakukan max absolute scaling pada kolom kategorikal. Tahap ini dilakukan karena: \n",
    "1. Menjaga sparsity data: Teknik ini menjaga sparsity data. Karena kolom yang diterapkan teknik ini adalah kolom one-hot yang cenderung sparse, maka teknik ini cocok digunakan.\n",
    "2. Menjaga relasi/hubungan antar fitur: Teknik ini tidak membuat fitur menjadi sebuah rentang tertentu, yang berarti hubungan antar fitur tetap terjaga.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_Lh-4JwLfHc"
   },
   "source": [
    "### II. Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "psElSUugLfHc"
   },
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "\n",
    "class OneHotCategorical(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self):\n",
    "    self.categorical_columns = []\n",
    "    self.onehot = None\n",
    "\n",
    "  def fit(self, X: pd.DataFrame, y):\n",
    "    self.onehot = sklearn.preprocessing.OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    self.onehot.fit(X)\n",
    "    return self\n",
    "  \n",
    "  def transform(self, X: pd.DataFrame):\n",
    "    return pd.DataFrame(self.onehot.transform(X), columns=self.onehot.get_feature_names_out(), index=X.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "Pada tahap ini dilakukan one hot encoding pada kolom kategorikal. Tahap one hot encoding dilakukan karena:\n",
    "1. Mengubah nilai kategorik menjadi numerik: Banyak algoritma machine learning membutuhkan fitur numerikal. Tahap one-hot encoding mengubah nilai-nilai pada kolom kategorik menjadi numerik dengan tetap mempertahankan keunikan nilai.\n",
    "2. Mencegah kesalahan pada interpretasi nilai ordinal: One hot encoding menjaga independensi antar nilai kategorikal, dan mencegah kesalahan interpretasi nilai ordinal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKQO9wtB8Pc0"
   },
   "source": [
    "### III. Handling Imbalanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "u2BQd2XJ9W1i"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "class SMOTEImbalance(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.smote = None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.DataFrame):\n",
    "        self.smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "        self.smote.fit_resample(X, y) \n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame, y: pd.DataFrame):\n",
    "        X_resampled, y_resampled = self.smote.fit_resample(X, y)\n",
    "        return pd.DataFrame(X_resampled, columns=X.columns), pd.DataFrame(y_resampled, columns=y.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE\n",
    "Pada block kode ini kami menyiapkan kelas SMOTE untuk menangani label target yang imbalanced dengan melakukan oversampling pada data label yang underrepresented. Meskipun pada akhirnya kelas ini tidak kami masukkan ke pipeline final, kelas ini membantu kami dalam melakukan tuning untuk menemukan pipeline terbaik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTMSyUzf_-HQ"
   },
   "source": [
    "### IV. Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "HLOWHftjF1JU"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class StandardScaleNumerical(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, with_mean=True):\n",
    "    self.with_mean = with_mean\n",
    "    self.scaler = None\n",
    "\n",
    "  def fit(self, X: pd.DataFrame, y):\n",
    "    self.scaler = StandardScaler(with_mean=self.with_mean)\n",
    "    self.scaler.fit(X)\n",
    "\n",
    "    return self\n",
    "\n",
    "  def transform(self, X: pd.DataFrame):\n",
    "    return pd.DataFrame(self.scaler.transform(X), columns=self.scaler.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaling\n",
    "Pada tahap ini dilakukan standard scaling pada kolom kategorik dan numerik. Tahap standard scaling dilakukan karena:\n",
    "1. Meratakan kontribusi setiap fitur pada proses jalannya model: Proses standard scaling akan membuat kontribusi setiap fitur sama dengan mengubah rentang setiap fitur sesuai dengan Z-score masing-masing.\n",
    "2. Menjaga relasi antar fitur: Tahap standard scaling menjaga relasi antar fitur karena standard scaling tidak melakukan mapping dari setiap nilai ke sebuah rentang spesifik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSZPZSntF3Kb"
   },
   "source": [
    "### V. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "5rksSMAWICY_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ctVzt5DLfHd"
   },
   "source": [
    "# 3. Compile Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "jHraoW_7LfHd"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', ImputeNumerical()),\n",
    "    ('scaler', MinMaxFeatureScaling()),\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', ImputeCategorical()),\n",
    "    ('onehot', OneHotCategorical()),\n",
    "    ('scaler', MaxAbsFeatureScaling()),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('numerical', numerical_pipeline, [*numerical_features, *binary_features]),\n",
    "    ('categorical', categorical_pipeline, [feature for feature in categorical_features if (feature != 'attack_cat')]),\n",
    "])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('filter', SelectKBest(score_func=f_classif, k=70)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9s56aFFxLfHd"
   },
   "outputs": [],
   "source": [
    "train_set = pipe.fit_transform(X_train, y_train)\n",
    "val_set = pipe.transform(X_val)\n",
    "print(train_set.shape)\n",
    "print(val_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9A3adbZXLfHe"
   },
   "source": [
    "# 4. Modeling and Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnhMNbBILfHf"
   },
   "source": [
    "## A. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KV6ICmFmlqjk"
   },
   "outputs": [],
   "source": [
    "# Type your code here\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(train_set, y_train)\n",
    "\n",
    "scores = cross_val_score(knn, train_set, y_train, cv=kf, scoring='accuracy')\n",
    "print(f\"Cross validation scores: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean()}\")\n",
    "print(f\"Standard deviation of accuracy: {scores.std()}\")\n",
    "\n",
    "y_pred = knn.predict(val_set)\n",
    "f1 = f1_score(y_val, y_pred, average='macro')\n",
    "print(f\"F1 score: {f1}\")\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['attack_cat'] = y_val\n",
    "df['predicted'] = y_pred\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class KNN(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, k=5, distance_metric='euclidean', p=3):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.p = p\n",
    "\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "        valid_metrics = ['euclidean', 'manhattan', 'minkowski']\n",
    "        if self.distance_metric.lower() not in valid_metrics:\n",
    "            raise ValueError(f\"Unsupported distance metric: {self.distance_metric}\")\n",
    "\n",
    "        self._X_train = None\n",
    "        self._y_train = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self._X_train = np.asarray(X)\n",
    "        self._y_train = self.label_encoder.fit_transform(y)\n",
    "        return self\n",
    "\n",
    "    def _chunked_pairwise_distances(self, X, chunk_size=500):\n",
    "        n_samples = X.shape[0]\n",
    "        n_train_samples = self._X_train.shape[0]\n",
    "        distances = np.zeros((n_samples, n_train_samples), dtype=np.float32)\n",
    "\n",
    "        for start in range(0, n_train_samples, chunk_size):\n",
    "            end = min(start + chunk_size, n_train_samples)\n",
    "            X_train_chunk = self._X_train[start:end]\n",
    "\n",
    "            if self.distance_metric.lower() == 'euclidean':\n",
    "                dists = np.sqrt(\n",
    "                    np.sum(X[:, np.newaxis, :] ** 2, axis=2) +\n",
    "                    np.sum(X_train_chunk ** 2, axis=1) -\n",
    "                    2 * np.dot(X, X_train_chunk.T)\n",
    "                )\n",
    "            elif self.distance_metric.lower() == 'manhattan':\n",
    "                dists = np.sum(\n",
    "                    np.abs(X[:, np.newaxis, :] - X_train_chunk), axis=2\n",
    "                )\n",
    "            elif self.distance_metric.lower() == 'minkowski':\n",
    "                dists = np.power(\n",
    "                    np.sum(\n",
    "                        np.abs(X[:, np.newaxis, :] - X_train_chunk) ** self.p, axis=2\n",
    "                    ),\n",
    "                    1 / self.p\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported distance metric: {self.distance_metric}\")\n",
    "\n",
    "            distances[:, start:end] = dists\n",
    "\n",
    "        return distances\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        batch_size = 1000\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            batch = X[i:i + batch_size]\n",
    "\n",
    "            distances = self._chunked_pairwise_distances(batch)\n",
    "\n",
    "            k_nearest_indices = np.argpartition(distances, self.k, axis=1)[:, :self.k]\n",
    "\n",
    "            k_nearest_labels = self._y_train[k_nearest_indices]\n",
    "\n",
    "            batch_predictions = np.apply_along_axis(lambda x: np.bincount(x).argmax(), 1, k_nearest_labels)\n",
    "\n",
    "            predictions.append(batch_predictions)\n",
    "\n",
    "        encoded_predictions = np.concatenate(predictions)\n",
    "        return self.label_encoder.inverse_transform(encoded_predictions)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNN(k=131, distance_metric=\"manhattan\")\n",
    "knn_model.fit(train_set, y_train)\n",
    "\n",
    "scores = cross_val_score(knn_model, train_set, y_train, cv=kf, scoring='accuracy', n_jobs=10, verbose=1)\n",
    "print(f\"Cross validation scores: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean()}\")\n",
    "print(f\"Standard deviation of accuracy: {scores.std()}\")\n",
    "\n",
    "y_pred = knn_model.predict(val_set)\n",
    "f1 = f1_score(y_val, y_pred, average='macro')\n",
    "print(f\"F1 score: {f1}\")\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['attack_cat'] = y_val\n",
    "df['predicted'] = y_pred\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nW0bMzkDLfHf"
   },
   "source": [
    "## B. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_XwsN_-LfHg"
   },
   "outputs": [],
   "source": [
    "# Type your code here\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(train_set, y_train)\n",
    "\n",
    "scores = cross_val_score(nb, train_set, y_train, cv=kf, scoring='accuracy')\n",
    "print(f\"Cross validation scores: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean()}\")\n",
    "print(f\"Standard deviation of accuracy: {scores.std()}\")\n",
    "\n",
    "y_pred = nb.predict(val_set)\n",
    "f1 = f1_score(y_val, y_pred, average='macro')\n",
    "print(f\"F1 score: {f1}\")\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['attack_cat'] = y_val\n",
    "df['predicted'] = y_pred\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class CustomNaiveBayes(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.class_priors = None\n",
    "        self.class_means = None\n",
    "        self.class_variances = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        self.class_means = np.zeros((n_classes, n_features))\n",
    "        self.class_variances = np.zeros((n_classes, n_features))\n",
    "        self.class_priors = np.zeros(n_classes)\n",
    "\n",
    "        for i, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "            self.class_priors[i] = X_c.shape[0] / X.shape[0]\n",
    "            self.class_means[i, :] = X_c.mean(axis=0)\n",
    "            self.class_variances[i, :] = X_c.var(axis=0) + 1e-7\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _gaussian_probability(self, x, mean, variance):\n",
    "        exponent = np.exp(-((x - mean)**2 / (2 * variance)))\n",
    "        return (1 / (np.sqrt(2 * np.pi * variance))) * exponent\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        n_classes = len(self.classes)\n",
    "        \n",
    "        probabilities = np.zeros((n_samples, n_classes))\n",
    "        for i in range(n_classes):\n",
    "            prior = np.log(self.class_priors[i])\n",
    "            conditional = np.sum(np.log(\n",
    "                self._gaussian_probability(\n",
    "                    X, \n",
    "                    self.class_means[i, :], \n",
    "                    self.class_variances[i, :]\n",
    "                ) + 1e-10 \n",
    "            ), axis=1)\n",
    "            \n",
    "            \n",
    "            probabilities[:, i] = prior + conditional\n",
    "        \n",
    "        probabilities = np.exp(probabilities)\n",
    "        probabilities /= probabilities.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return self.classes[np.argmax(probabilities, axis=1)]\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomNaiveBayes()\n",
    "model.fit(train_set, y_train)\n",
    "\n",
    "scores = cross_val_score(model, train_set, y_train, cv=kf, scoring='accuracy')\n",
    "print(f\"Cross validation scores: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean()}\")\n",
    "print(f\"Standard deviation of accuracy: {scores.std()}\")\n",
    "\n",
    "y_pred = model.predict(val_set)\n",
    "\n",
    "f1 = f1_score(y_val, y_pred, average='macro')\n",
    "print(f\"F1 score: {f1}\")\n",
    "\n",
    "df = pd.DataFrame({'attack_cat': y_val, 'predicted': y_pred})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLDtIkPdLfHg"
   },
   "source": [
    "## C. ID3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZ6_x1LKLfHh"
   },
   "outputs": [],
   "source": [
    "# Type your code here\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "dt.fit(train_set, y_train)\n",
    "\n",
    "scores = cross_val_score(dt, train_set, y_train, cv=kf, scoring='accuracy')\n",
    "print(f\"Cross validation scores: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean()}\")\n",
    "print(f\"Standard deviation of accuracy: {scores.std()}\")\n",
    "\n",
    "y_pred = dt.predict(val_set)\n",
    "f1 = f1_score(y_val, y_pred, average='macro')\n",
    "print(f\"F1 score: {f1}\")\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['attack_cat'] = y_val\n",
    "df['predicted'] = y_pred\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class ID3(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, max_depth=5, n_bins=10):\n",
    "        self.max_depth = max_depth\n",
    "        self.n_bins = n_bins\n",
    "    \n",
    "    def _discretize(self, X):\n",
    "        X_binned = np.zeros_like(X, dtype=int)\n",
    "        for i in range(X.shape[1]):\n",
    "            bins = np.linspace(X[:, i].min(), X[:, i].max(), self.n_bins + 1)\n",
    "            X_binned[:, i] = np.digitize(X[:, i], bins[1:-1]) \n",
    "        return X_binned\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "        X, y = check_X_y(X, y, dtype=np.float64)\n",
    "        \n",
    "        X_binned = self._discretize(X)\n",
    "        \n",
    "        self.tree_ = self._build_tree(X_binned, y)\n",
    "        \n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        self.classes_ = le.classes_\n",
    "        self.label_encoder_ = le\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        return -np.sum(probabilities * np.log2(probabilities + 1e-10))\n",
    "    \n",
    "    def _information_gain(self, X, y, feature_idx):\n",
    "        total_entropy = self._entropy(y)\n",
    "        \n",
    "        unique_values = np.unique(X[:, feature_idx])\n",
    "        \n",
    "        weighted_entropies = np.array([\n",
    "            len(y[X[:, feature_idx] == value]) / len(y) * \n",
    "            self._entropy(y[X[:, feature_idx] == value])\n",
    "            for value in unique_values\n",
    "        ])\n",
    "        \n",
    "        return total_entropy - np.sum(weighted_entropies)\n",
    "    \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        unique_classes = np.unique(y)\n",
    "        \n",
    "        if (len(unique_classes) == 1 or \n",
    "            depth == self.max_depth or \n",
    "            X.shape[1] == 0):\n",
    "            return np.argmax(np.bincount(y)) if len(y) > 0 else None\n",
    "        \n",
    "        gains = np.array([\n",
    "            self._information_gain(X, y, i) \n",
    "            for i in range(X.shape[1])\n",
    "        ])\n",
    "        \n",
    "        best_feature = np.argmax(gains)\n",
    "        max_gain = gains[best_feature]\n",
    "        \n",
    "        if max_gain == 0:\n",
    "            return np.argmax(np.bincount(y))\n",
    "        \n",
    "        node = {'feature': best_feature}\n",
    "        node['children'] = {}\n",
    "        \n",
    "        unique_values = np.unique(X[:, best_feature])\n",
    "        \n",
    "        for value in unique_values:\n",
    "            mask = X[:, best_feature] == value\n",
    "            \n",
    "            X_subset = np.delete(X[mask], best_feature, axis=1)\n",
    "            y_subset = y[mask]\n",
    "            \n",
    "            if len(y_subset) == 0:\n",
    "                continue\n",
    "            \n",
    "            subtree = self._build_tree(\n",
    "                X_subset,\n",
    "                y_subset,\n",
    "                depth=depth+1\n",
    "            )\n",
    "            \n",
    "            node['children'][value] = subtree\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = check_array(X, dtype=np.float64)\n",
    "        check_is_fitted(self, ['tree_', 'classes_'])\n",
    "        \n",
    "        X_binned = self._discretize(X)\n",
    "        \n",
    "        predictions = np.array([self._predict_single(x) for x in X_binned])\n",
    "        return self.label_encoder_.inverse_transform(predictions)\n",
    "    \n",
    "    def _predict_single(self, x):\n",
    "        node = self.tree_\n",
    "        \n",
    "        while isinstance(node, dict):\n",
    "            feature = node['feature']\n",
    "            value = x[feature]\n",
    "            \n",
    "            node = node['children'].get(value, list(node['children'].values())[0])\n",
    "            x = np.delete(x, feature)\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X = check_array(X, dtype=np.float64)\n",
    "        check_is_fitted(self, ['tree_', 'classes_'])\n",
    "        \n",
    "        predictions = self.predict(X)\n",
    "        \n",
    "        proba = np.zeros((X.shape[0], len(self.classes_)))\n",
    "        proba[np.arange(len(predictions)), \n",
    "              self.label_encoder_.transform(predictions)] = 1\n",
    "        \n",
    "        return proba\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id3 = ID3(max_depth=70, n_bins=66)\n",
    "id3.fit(train_set, y_train)\n",
    "\n",
    "scores = cross_val_score(id3, train_set, y_train, cv=kf, scoring='accuracy', n_jobs=10, verbose=1)\n",
    "print(f\"Cross validation scores: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean()}\")\n",
    "print(f\"Standard deviation of accuracy: {scores.std()}\")\n",
    "\n",
    "y_pred = id3.predict(val_set)\n",
    "f1 = f1_score(y_val, y_pred, average='macro')\n",
    "print(f\"F1 score: {f1}\")\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['attack_cat'] = y_val\n",
    "df['predicted'] = y_pred\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoH2u6fOLfHh"
   },
   "source": [
    "## D. Improvements (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "pg-A54yELfHh"
   },
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Li4l53DjLfHh"
   },
   "source": [
    "## E. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LeqnfWc-LfHi"
   },
   "outputs": [],
   "source": [
    "# Membaca semua file csv test\n",
    "additional_features_df = pd.read_csv('../dataset/test/additional_features_test.csv')\n",
    "basic_features_df = pd.read_csv('../dataset/test/basic_features_test.csv')\n",
    "content_features_df = pd.read_csv('../dataset/test/content_features_test.csv')\n",
    "flow_features_df = pd.read_csv('../dataset/test/flow_features_test.csv')\n",
    "time_features_df = pd.read_csv('../dataset/test/time_features_test.csv')\n",
    "\n",
    "# Menggabungkan data training dan testing untuk analisis EDA menyeluruh\n",
    "test_data = pd.merge(basic_features_df, additional_features_df, on=\"id\")\n",
    "test_data = pd.merge(test_data, content_features_df, on=\"id\")\n",
    "test_data = pd.merge(test_data, flow_features_df, on=\"id\")\n",
    "test_data = pd.merge(test_data, time_features_df, on=\"id\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', ImputeNumerical()),\n",
    "    ('scaler', MinMaxFeatureScaling()),\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', ImputeCategorical()),\n",
    "    ('onehot', OneHotCategorical()),\n",
    "    ('scaler', MaxAbsFeatureScaling()),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('numerical', numerical_pipeline, [*numerical_features, *binary_features]),\n",
    "    ('categorical', categorical_pipeline, [feature for feature in categorical_features if (feature != 'attack_cat')]),\n",
    "])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('filter', SelectKBest(score_func=f_classif, k=70)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pipe.fit_transform(X, y)\n",
    "test_set = pipe.transform(test_data)\n",
    "print(train_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model.fit(train_set, y)\n",
    "with open('../model/knn.pkl', 'wb') as f:\n",
    "    pickle.dump(knn_model, f)\n",
    "\n",
    "y_pred = knn_model.predict(test_set)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['id'] = test_data['id']\n",
    "df['attack_cat'] = y_pred\n",
    "df.to_csv('../submission/knn.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_set, y)\n",
    "with open('../model/nb.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "\n",
    "y_pred = model.predict(test_set)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['id'] = test_data['id']\n",
    "df['attack_cat'] = y_pred\n",
    "df.to_csv('../submission/nb.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id3.fit(train_set, y)\n",
    "with open('../model/id3.pkl', 'wb') as model_file:\n",
    "    pickle.dump(id3, model_file)\n",
    "\n",
    "y_pred_id3 = id3.predict(test_set)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['id'] = test_data['id']\n",
    "df['attack_cat'] = y_pred_id3\n",
    "df.to_csv('../submission/id3_scratch_submission.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-jXvKOpLfHi"
   },
   "source": [
    "# 6. Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWL3nEAELfHj"
   },
   "source": [
    "### KNN\n",
    "Dari hasil perbandingan di atas dapat dilihat bahwa model yang diimplementasikan secara manual memiliki performa lebih buruk dibandingkan model dari library.\n",
    "Kami juga menemukan bahwa model ini sangat sensitif terhadap scaling, karena menggunakan min max scaling pada fitur numerikal dan max absolute scaling pada fitur kategorikal membantu performa model dengan signifikan, terlihat dari F1 score validasi yang meningkat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "Dari hasil perbandingan di atas, dapat dilihat bahwa nilai F1 score dari implementasi F1-score manual lebih buruk dibandingkan implementasi F1-score dari library. Hal ini karena implementasi library sudah di optimasi menggunakan teknik yang lebih advanced. \n",
    "Dari hasil eksperimentasi kami, kami juga menemukan bahwa missing values lebih baik diganti menggunakan teknik imputing dibandingkan di-drop. Hal ini karena banyak row yang memiliki missing values cukup banyak, kurang lebih ⅞ dari total data. Hal ini tentu akan memperkecil data training secara signifikan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3\n",
    "Sama seperti Naive Bayes, model yang diimplementasikan sendiri untuk ID3 memiliki performa lebih buruk dibandingkan model yang menggunakan library.\n",
    "Selain itu, kami juga menemukan bahwa proses imputing meningkatkan performa model. Hal ini karena dengan imputing dan tidak men-drop row data, data training yang model dapatkan semakin banyak, memungkinkan proses training yang lebih relevan ke data aslinya.\n",
    "Selain itu, feature scaling juga memperbaiki performa model, tepatnya Min Max Absolute Scaling pada kolom numerikal. Hal ini karena dengan scaling, didapat representasi fitur yang lebih uniform, dibanding tanpa scaling dimana setiap fitur memiliki rentangnya masing-masing.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
